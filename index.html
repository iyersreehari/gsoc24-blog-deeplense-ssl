<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learning Representation Through Self-Supervised Learning on Real Gravitational Lensing Images</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <title>Table with Divs</title>
    <style>
        .table {
            display: table;
            width: 80%;
            border-collapse: collapse;
            margin: 20px auto;
        }
        .table-row {
            display: table-row;
        }
        .table-header, .table-cell {
            display: table-cell;
            border: 1px solid black;
            padding: 8px;
            text-align: center;
        }
        .table-header {
            background-color: #f2f2f2;
            font-weight: bold;
        }
    </style>
    <!-- Include MathJax library -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
      .image-row {
          display: flex;
          justify-content: space-around;
      }
      .image-container {
          text-align: center;
          width: 45%; /* Adjust the width to fit your design */
      }
      .image-caption {
          margin-top: 8px;
          font-style: italic;
          color:#555;
      }
  </style>
	<title>Image with Caption</title>
    <style>
        .image-container {
            text-align: center;
            margin: 20px;
        }
        .image-caption {
            margin-top: 10px;
            font-style: italic;
            color: #555;
        }
    </style>
    
    <title>Citations</title>
    <style>
        .citation {
            font-style: italic;
        }
        .references {
            margin-top: 20px;
        }
        .reference {
            margin-bottom: 10px;
        }
    </style>

    <title>Blue Links</title>
    <style>
        a {
            color: blue;
        }
    </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning Representation Through Self-Supervised Learning on Real Gravitational Lensing Images</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="AUTHOR PERSONAL LINK" target="iyersreehari.github.io">Sreehari Iyer</a></span>
            </div>

                <div class="is-size-5 publication-authors">
                    <span class="author-block">Indian Institute of Technology Madras<br>
                </span> </div>

                  
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
This blog post provides an overview of my ongoing project with <a href = "https://ml4sci.org"><u>Machine Learning for Science (ML4Sci)</u></a> as part of the <a href = "https://summerofcode.withgoogle.com/programs/2024"><u>Google Summer of Code (GSoC) 2024</u></a>. All the project's code is openly accessible at <a href = "https://github.com/iyersreehari/DeepLense_SSL_Sreehari_Iyer"><u>github.com/iyersreehari/DeepLense_SSL_Sreehari_Iyer</u></a><br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Intro -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Strong gravitational lensing provides a means to probe dark matter substructure. In recent years, machine learning techniques, particularly supervised learning, are being utilized for substructure detection, and other regression and classification tasks on the lensing dataset. With strong gravitational lensing, the available labeled training data is scarce. Supervised learning requires abundant labeled training data and can be biased by class imbalances in the training dataset. To circumvent this, previous works have utilized simulated lensing dataset for supervised learning. However, this approach may result in diminished performance on running inferences with real datasets. In computer vision, self-supervised learning (SSL) has emerged as a potent solution, particularly effective in scenarios with abundant unlabeled data and scarce labeled data. Recent works have studied convolutional neural network (CNN) based SSL with simulated lensing data. This project focuses on evaluating of self-supervised learning techniques with Transformers utilizing real-world strong gravitational lensing dataset.<br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!--  Intro End -->

<!-- Experiment Details -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-justified">
          <p> This project focuses on evaluating of self-supervised learning techniques with Transformers utilizing real-world strong gravitational lensing dataset. The learned representations are then evaluated on the downstream task to classify lens and non-lens images. 
            We train and compare Vision Transformer [<a href="#ref1" class="citation"><u>Dosovitskiy, Alexey, et al., 2020</u></a>] with different self supervised learning algorithms alongwith a supervised baseline. Vision Transformers split the image into fixed-size patches and linearly embed them with additional position embedding. The resulting vector representation is then fed into a standard Transformer
encoder. The following llustration of the ViT architecture is from <a href="#ref1" class="citation"><u>Dosovitskiy, Alexey, et al., 2020</u></a>
<div class="image-container">
          <img src="static/images/vit_architecture.png" alt="ViT Architecture" style="max-width: 100%;">
          <div class="image-caption">ViT Architecture from <a href="#ref1" class="citation"><u>Dosovitskiy, Alexey, et al., 2020</u></a></div>
      </div>
For the supervised learning baseline, the ViT backbone is followed by a classifier MLP head. The hyperparameters for the classification task are chosen such that the loss computed over the validation dataset is minimized. For self-supervised learning, the ViT backbone is trained on the training dataset without the label information. The hyperparameters are chosen such that the K-NN accuracy computed over the representations obtained for the validation dataset is maximized. For evaluating the learned network, the backbone followed by a linear classifier is finetuned on the train dataset (with label information) and then evaluated over the held-out test dataset. All optimizations are performed using the Adam optimizer, with cosine annealing learning rate scheduler with warm restarts.
<div class="image-container">
          <img src="static/images/sample_image_dataset.png" alt="Example images from the training dataset" style="max-width: 100%;">
          <div class="image-caption">Example images from the training dataset</div>
      </div>
Each image has three channels or filters - b (blue filter), g (green filter) and i (near infrared filter). Detailed information about the filters are available at <a href="https://skyserver.sdss.org/dr1/en/proj/advanced/color/sdssfilters.asp"><u> SDSS Filters</u></a> <br>
The dataset contains 1949 lens images and 2000 non-lens images. The dataset is split in a stratified manner into train dataset containing 85 percentage of the dataset, and test dataset containing the rest 15 percentage of the dataset.<br>
The images are center cropped to 32 × 32 pixel as this empirically resulted in better prediction accuracy. 
To understand how well SSL works with different fractions of labelled and unlabelled data, the models are pre-trained through self supervision on the entire data and then finetuned on the labelled fraction and compared with supervised baseline trained only on that labeled fraction. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Experiment Details End -->


<!-- Supervised Baseline -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Supervised Learning Baseline</h2>
        <div class="content has-text-justified">
          <p>
            A supervised learning baseline is trained to compare the performance of self-supervised learning algorithms. 
            The ViT-B and ViT-S networks are trained to minimize the cross entropy loss between the predicted labels and the actual labels for the training dataset (with the label information).  
	  </p>
      </div>
      <div class="table">
            <div class="table-row">
                <div class="table-header">Backbone</div>
                <div class="table-header"># labelled data for<br>training/fine-tuning</div>
                <div class="table-header">Accuracy</div>
                <div class="table-header">AUC Score</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-S (patch size: 8)</div>
                <div class="table-cell">300</div>
                <div class="table-cell">86.0034%</div>
                <div class="table-cell">0.9235</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 8)</div>
                <div class="table-cell">300</div>
                <div class="table-cell">86.6779%</div>
                <div class="table-cell">0.9339</div>
            </div>
            <div class="table-row">
              <div class="table-cell">ViT-S (patch size: 8)</div>
              <div class="table-cell">600</div>
              <div class="table-cell">88.0270%</div>
              <div class="table-cell">0.9210</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 8)</div>
                <div class="table-cell">600</div>
                <div class="table-cell">88.3642%</div>
                <div class="table-cell">0.9371</div>
            </div>
            <div class="table-row"></div>
              <div class="table-cell">ViT-S (patch size: 8)</div>
              <div class="table-cell">1200</div>
              <div class="table-cell">89.5447%</div>
              <div class="table-cell">0.9663</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 8)</div>
                <div class="table-cell">1200</div>
                <div class="table-cell">90.7251%</div>
                <div class="table-cell">0.9698</div>
            </div>
            <div class="table-row"></div>
              <div class="table-cell">ViT-S (patch size: 8)</div>
              <div class="table-cell">3256</div>
              <div class="table-cell">93.7605%</div>
              <div class="table-cell">0.9786</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 8)</div>
                <div class="table-cell">3256</div>
                <div class="table-cell">93.0860%</div>
                <div class="table-cell">0.9786</div>
            </div>
        </div>

      <div class="image-row">
        <div class="image-container">
            <img src="static/images/auc_supervised_vit_small.png" alt="Supervised Learning Baseline" style="max-width: 100%;">
            <div class="image-caption">ROC for Supervised Learning <br> with ViT-S</div>
        </div>
    
        <div class="image-container">
            <img src="static/images/auc_supervised_vit_base.png" alt="Unsupervised Learning Baseline" style="max-width: 100%;">
            <div class="image-caption">ROC for Supervised Learning <br> with ViT-B</div>
        </div>
      </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- Supervised Baseline End -->

<!-- SimSiam -->

<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">SimSiam </h2>
	      <h4 style="display:inline"><a href="#ref2" class="citation">[<u>Chen, Xinlei, and Kaiming He., 2021</u>]</a></h4>
        <div class="content has-text-justified">
          <p>
            Recent studies in self-supervised representation learning,
            utilize methods involving certain forms of Siamese networks.  
          </p>
          <div class="image-container">
          <img src="static/images/simsiam_architecture.png" alt="SimSiam Architecture" style="max-width: 100%;">
          <div class="image-caption">SimSiam Architecture from <a href="#ref2" class="citation"><u>Chen, Xinlei, and Kaiming He., 2021</u></a></div>
        </div>
          <p>
              The SimSiam architecture takes as input two randomly
augmented views \( x_1 \) and \( x_2 \) from an image \( x \). We utilize random horizontal flips, random brightness and contrast jitter and random gaussian blur for obtaining the augmentations. <br> The architecture consists an encoder network \( f \) followed by a prediction MLP head \( h \). The encoder network \( f \) consists of the backbone network followed by a projection MLP. <br>
    Let the cosine similarity between the output vectors \( p_1 = h(f(x_1)) \) and \( z_2 = f(x_2) \) be \[ \mathcal{D} (p_1, z_2) = \frac{p_1}{|| p_1 ||_2} \cdot \frac{z_2}{|| z_2 ||_2} \]
    The objective of SimSiam is to minimize the symmetrized loss defined as follows:
    \[ \mathcal{L} = - \frac{1}{2} \mathcal{D} (p_1, z_2) - \frac{1}{2} \mathcal{D} (p_2, z_1) \]
    Further, the architecture utilizes stop-gradient operation to avoid representation collapse which may cause the optimizer to quickly obtain a degenerated solution reaching the minimum possible loss of −1. <br>
    The evaluation results on the test dataset after finetuning the architecture on the train dataset are given below
          </p>
      </div>
      <div class="table">
            <div class="table-row">
                <div class="table-header">Model</div>
                <div class="table-header"># Parameters</div>
                <div class="table-header">Accuracy</div>
                <div class="table-header">AUC Score</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-S (patch size: 16)</div>
                <div class="table-cell">22M</div>
                <div class="table-cell">91.5683 %</div>
                <div class="table-cell">0.9703</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 16)</div>
                <div class="table-cell">86M</div>
                <div class="table-cell">91.5683 %</div>
                <div class="table-cell">0.9651</div>
            </div>
        </div>
      <div class="image-container">
          <img src="static/images/cm_roc_simSiam.png" alt="SimSiam Evaluation" style="max-width: 100%;">
          <div class="image-caption">Confusion Matrix and ROC for SimSiam</div>
      </div>
      </div>
      </div>
    </div>
  </div>
</section> -->
<!-- SimSiam End -->
	
<!-- dino -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">DINO</h2>
	      <h4 style="display:inline"><a href="#ref3" class="citation">[<u>Caron, Mathilde, et al., 2021</u>]</a></h4>
        <div class="content has-text-justified">         
          <!-- Simsiam architecture image -->
          <div class="image-container">
          <img src="static/images/dino_architecture.png" alt="DINO Architecture" style="max-width: 100%;">
          <div class="image-caption">DINO Architecture from <a href="#ref3" class="citation"><u>Caron, Mathilde, et al., 2021</u></a></div>
        </div>
          <p>
              The model consists of student and teacher networks.
The output of the teacher network is centered with mean computed over a batch.
The output of the student and the teacher networks are normalized with a temperature softmax over the feature dimension. <br>
<br>
[<a href="#ref4" class="citation"><u>Zhou, Jinghao, et al., 2021</u></a>] Given the training set \( \mathcal{I} \), an image \( x \sim \mathcal{I} \) is sampled uniformly, over
which two random augmentations are applied, yielding two distorted views \( u \) and \( v \). We utilize random horizontal flips, random brightness and contrast jitter and random gaussian blur for obtaining the augmentations. 
		  The two distorted views are then put through a teacher-student framework to get the predictive categorical distributions from the \( \texttt{[CLS]} \) token:
\[
v^{\texttt{[CLS]}}_t = P_{\theta '}^{\texttt{[CLS]}} (v) \
\text{ and } \
u^{\texttt{[CLS]}}_s = P_{\theta}^{\texttt{[CLS]}} (u)
\]
<br>
where \( \theta ' \) denotes the parameters of the teacher network and \( \theta \) denotes the parameters of the student network. <br>
The knowledge is distilled from teacher to student by minimizing their cross-entropy loss w.r.t. the student parameters \( \theta_s \) <br>
\[
\mathcal{L}_{\texttt{[CLS]}} (u, v) = - P_{\theta '}^{\texttt{[CLS]}} (v) \cdot \log {(P_{\theta}^{\texttt{[CLS]}} (u))}
\]
The teacher and the student share the same architecture consisting of a backbone \( \mathcal{f} \) 
and a projection MLP head \( h \). A stop-gradient (sg) operator is applied on the teacher network similar to SimSiam architecture.
<br> <br>
Self-supervised learning is implenmented in <a href="#ref3" class="citation"><u>Caron, Mathilde, et al., 2021</u></a> through different distorted views, or crops, of an image with multi-crop strategy. For a given image, a set \( V \) of different views is generated. This set contains two (or more) global views, \( x_{g1} \) and \( x_{g2} \) and several local views of smaller resolution (image cropped to cover smaller area). All crops are passed through the student network while only the global views are passed through the teacher network. <br>
Thus, the objective to minimize w.r.t. the student parameter \( \theta_s \) is as follows:
\[
\mathcal{L}_{DINO} =  \sum_{v \in {x_{g1}, x_{g2}}} \ \sum_{u \in V, u \neq v} 
- P_{\theta '}^{\texttt{[CLS]}} (v) \cdot \log {(P_{\theta}^{\texttt{[CLS]}} (u))}
\]
The teacher parameters \( \theta_t \) are updated with
an exponential moving average (ema) of the student parameters \( \theta_s \).

    The evaluation results on the test dataset after finetuning the architecture on the train dataset are given below 
          </p>
      </div>
      <div class="table">
            <div class="table-row">
                <div class="table-header">Model</div>
                <div class="table-header"># Parameters</div>
                <div class="table-header">Accuracy</div>
                <div class="table-header">AUC Score</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-S (patch size: 16)</div>
                <div class="table-cell">22M</div>
                <div class="table-cell">93.9292 %</div>
                <div class="table-cell">0.9782</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 16)</div>
                <div class="table-cell">86M</div>
                <div class="table-cell">92.9174 %</div>
                <div class="table-cell">0.9785</div>
            </div>
        </div>
        <div class="image-container">
          <img src="static/images/cm_roc_dino.png" alt="DINO Evaluation" style="max-width: 100%;">
          <div class="image-caption">Confusion Matrix and ROC for DINO</div>
      </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- dino End -->

<!-- ibot -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">iBOT</h2>
	      <h4 style="display:inline"><a href="#ref4" class="citation">[<u>Zhou, Jinghao, et al., 2021</u>]</a></h4>
        <div class="content has-text-justified">         
         <!-- Your image here -->
         <div class="image-container">
          <img src="static/images/ibot_architecture.png" alt="iBot Architecture" style="max-width: 100%;">
          <div class="image-caption">iBot Architecture from <a href="#ref4" class="citation">[<u>Zhou, Jinghao, et al., 2021</u>]</a></div>
      </div>
     </div>
     <div class="content has-text-justified">
          <p>
            iBot objective is a linear combination of the DINO loss and the Masked Image Modelling (MIM) loss.
            \[
            \lambda_1 \cdot \mathcal{L}_{DINO} + \lambda_2 \cdot \mathcal{L}_{MIM}
            \]
            where \( \mathcal{L}_{DINO} \) is the objective to be minimized in DINO as described in the previous section and \( \lambda_1 \), \( \lambda_2 \) are hyperparameters.<br>
            The objective is minimized with respect to \( \theta_s \), the parameters of the student network. The parameters of the teacher network \( \theta_t \) are updated with an exponential moving average (ema) of the student parameters. To compute the MIM loss, a blockwise mask is applied to the two augmented views \( x_1 \), \( x_2 \) of the same image \( x \) and their corresponding masked views \( \hat{x}_1 \) and \( \hat{x}_2 \) are obtained. This is achieved by masking random contiguous blocks of image patches, effectively covering square-shaped regions with continuous blocks.   
The training objective of MIM in iBOT is defined as:
\[
\mathcal{L}_{MIM} = - \sum_{i=1}^N {m_i \cdot P_{\theta'}^{patch} (x^i_1) \cdot log (P_{\theta}^{patch} (\hat{x}^i_1)) } - \sum_{i=1}^N {m_i \cdot P_{\theta'}^{patch} (x^i_2) \cdot log (P_{\theta}^{patch} (\hat{x}^i_2)) }
\]
Here, \( N \) is the number of images in the train dataset. While implementing the self-supervised learning through multi-crop strategy as described in the previous section, the terms \( x_1 \) and \( x_2 \) correspond to the global views. 
            We again use random horizontal flips, random brightness and contrast jitter and random gaussian blur for obtaining the augmentations. 
          </p>
      </div>
      <div class="table">
            <div class="table-row">
                <div class="table-header">Model</div>
                <div class="table-header"># Parameters</div>
                <div class="table-header">Accuracy</div>
                <div class="table-header">AUC Score</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-S (patch size: 16)</div>
                <div class="table-cell">22M</div>
                <div class="table-cell">91.7369 %</div>
                <div class="table-cell">0.9720</div>
            </div>
            <div class="table-row">
                <div class="table-cell">ViT-B (patch size: 16)</div>
                <div class="table-cell">86M</div>
                <div class="table-cell">91.0624 %</div>
                <div class="table-cell">0.9689</div>
            </div>
        </div>
        <div class="image-container">
          <img src="static/images/cm_roc_ibot.png" alt="iBot Evaluation" style="max-width: 100%;">
          <div class="image-caption">Confusion Matrix and ROC for iBot</div>
      </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- ibot End -->


<!-- conclusion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
	<div class="content has-text-justified"> 
	In this blog, I have presented some of the SSL algorithms that have been evaluated on the lensing dataset.
	The SSL algorithms have a marginal improvement over the supervised baseline. This may be due to the small size of the dataset.
	</div>
      </div>
    </div>
  </div>
</section>
<!-- ibot End -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
	<h2 class="title is-3">References</h2>
	<div class="content has-text-justified"> 
	<div class="references">
	    <div class="reference" id="ref1">
	        Dosovitskiy, Alexey, et al. <i>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</i> International Conference on Learning Representations. 2020.
	    </div>
	    <div class="reference" id="ref2">
	        Chen, Xinlei, and Kaiming He. <i>Exploring simple siamese representation learning.</i> Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.
	    </div>
	    <div class="reference" id="ref3">
	        Caron, Mathilde, et al. <i>Emerging properties in self-supervised vision transformers.</i> Proceedings of the IEEE/CVF international conference on computer vision. 2021.
	    </div>
	    <div class="reference" id="ref4">
	        Zhou, Jinghao, et al. <i>ibot: Image bert pre-training with online tokenizer.</i> arXiv preprint arXiv:2111.07832 (2021).
	    </div> 
	</div>
	</div>
      </div>
    </div>
  </div>
</section>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
